{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exR54L_OnZnF"
      },
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5PTQ0NpndIs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j56N9kBnVKe"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "HA3ZMXAanQjV"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm.auto import tqdm\n",
        "from abc import ABC, abstractmethod\n",
        "from transformers import BertTokenizerFast, DataCollatorWithPadding, PreTrainedModel, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_metric\n",
        "import pandas as pd\n",
        "import os\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJqFZTiHSxYp"
      },
      "source": [
        "# Data Reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "EXlXXPPbSw_V"
      },
      "outputs": [],
      "source": [
        "class DataReader:\n",
        "    def __init__(self, data_folder, output_file):\n",
        "        self.data_folder = data_folder\n",
        "        self.output_file = output_file\n",
        "\n",
        "    def process(self):\n",
        "        files = [pd.read_xml(os.path.join(self.data_folder, file)) for file in os.listdir(self.data_folder) if file.endswith(\".xml\")]\n",
        "        data = pd.concat(files, ignore_index=True)\n",
        "        data.rename(columns={'t1': 'premise', 't2': 'hypothesis'}, inplace=True)\n",
        "        \n",
        "        # Convert unique string labels to integers\n",
        "        unique_labels = data['label'].unique()\n",
        "        data['labels'] = data['label']\n",
        "        label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        data['label'] = data['label'].map(label_to_int)\n",
        "        \n",
        "        data.dropna(inplace=True)\n",
        "        data.reset_index(drop=True, inplace=True)\n",
        "        data.to_csv(self.output_file, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMBlR6g0neLM"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "zCsyWA_Fndsm"
      },
      "outputs": [],
      "source": [
        "class DataProcessor(ABC):\n",
        "    def __init__(self, tokenizer, config):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config\n",
        "\n",
        "    @abstractmethod\n",
        "    def tokenize_and_cut(self, sentence):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def preprocess(self, premise, hypothesis, label):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def collate_fn(self, batch):\n",
        "        pass\n",
        "\n",
        "    def split_dataset(self, dataset, train_val_ratio=0.9):\n",
        "        train_val_split_idx = int(len(dataset) * train_val_ratio)\n",
        "        train_val_dataset, test_dataset = dataset[:train_val_split_idx], dataset[train_val_split_idx:]\n",
        "        \n",
        "        train_split_idx = int(len(train_val_dataset) * train_val_ratio)\n",
        "        train_dataset, val_dataset = train_val_dataset[:train_split_idx], train_val_dataset[train_split_idx:]\n",
        "        \n",
        "        return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "    def get_data_loaders(self, csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "        dataset = [{\"premise\": row[\"premise\"], \"hypothesis\": row[\"hypothesis\"], \"label\": row[\"label\"]} for _, row in df.iterrows()]\n",
        "        dataset = [self.preprocess(data[\"premise\"], data[\"hypothesis\"], data[\"label\"]) for data in dataset]\n",
        "\n",
        "        \n",
        "        train_dataset, val_dataset, test_dataset = self.split_dataset(dataset)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "class BiLSTMDataProcessor(DataProcessor):\n",
        "    def __init__(self, tokenizer, embedding, config):\n",
        "        super().__init__(tokenizer, config)\n",
        "        self.embedding = embedding\n",
        "\n",
        "    def tokenize_and_cut(self, sentence):\n",
        "        tokens = self.tokenizer(sentence)\n",
        "        tokens = tokens[:self.config.max_length-2]\n",
        "        return tokens\n",
        "\n",
        "    def preprocess(self, premise, hypothesis, label):\n",
        "        # Convert tokens to their respective indices\n",
        "        premise = [self.embedding.stoi.get(token, self.embedding.stoi.get(\"<unk>\", 0)) for token in self.tokenize_and_cut(premise)]\n",
        "        hypothesis = [self.embedding.stoi.get(token, self.embedding.stoi.get(\"<unk>\", 0)) for token in self.tokenize_and_cut(hypothesis)]\n",
        "        \n",
        "        return {\n",
        "            \"premise\": torch.LongTensor(premise),\n",
        "            \"hypothesis\": torch.LongTensor(hypothesis),\n",
        "            \"labels\": label\n",
        "        }\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        premise = pad_sequence([data['premise'] for data in batch], batch_first=True)\n",
        "        hypothesis = pad_sequence([data['hypothesis'] for data in batch], batch_first=True)\n",
        "        labels = torch.tensor([data['labels'] for data in batch])\n",
        "        return {\"premise\": premise, \"hypothesis\": hypothesis, \"labels\": labels}\n",
        "\n",
        "\n",
        "class BERTDataProcessor(DataProcessor):\n",
        "    def __init__(self, tokenizer, config):\n",
        "        super().__init__(tokenizer, config)\n",
        "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "\n",
        "    def tokenize_and_cut(self, premise, hypothesis):\n",
        "        tokens = self.tokenizer(premise, hypothesis,\n",
        "                                max_length=self.config.max_length,\n",
        "                                truncation=True)\n",
        "        return tokens\n",
        "\n",
        "    def preprocess(self, premise, hypothesis, label):\n",
        "        tokens = self.tokenize_and_cut(premise, hypothesis)\n",
        "        tokens[\"labels\"] = label\n",
        "        return tokens\n",
        "\n",
        "    def get_data_loaders(self, csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "        dataset = [{\"premise\": row[\"premise\"], \"hypothesis\": row[\"hypothesis\"], \"label\": row[\"label\"]} for _, row in df.iterrows()]\n",
        "        dataset = [self.preprocess(data[\"premise\"], data[\"hypothesis\"], data[\"label\"]) for data in dataset]\n",
        "\n",
        "        # split the dataset into training, validation and test sets\n",
        "\n",
        "        train_dataset, val_dataset, test_dataset =  self.split_dataset(dataset)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True, collate_fn=self.data_collator)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size, shuffle=False, collate_fn=self.data_collator)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False, collate_fn=self.data_collator)\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrnMUVh0ThPy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYzpJf-4npkP"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "oxIboy5MnsOb"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "class BiLSTM(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, options):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        set_seed(options['seed_value'])\n",
        "        self.embed_dim = 300\n",
        "        self.hidden_size = options['d_hidden']\n",
        "        self.num_classes = options['out_dim']\n",
        "        self.directions = 2\n",
        "        self.num_layers = 2\n",
        "        self.concat = 4\n",
        "        self.device = options['device']\n",
        "        # Embedding layer\n",
        "        self.embedding =  torch.nn.Embedding(vocab_size, self.embed_dim)\n",
        "        self.projection = torch.nn.Linear(self.embed_dim, self.hidden_size)\n",
        "        self.lstm = torch.nn.LSTM(self.hidden_size, self.hidden_size, self.num_layers,\n",
        "                            bidirectional=True, batch_first=True, dropout=options['dp_ratio'])\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(p=options['dp_ratio'])\n",
        "\n",
        "        self.lin1 = torch.nn.Linear(self.hidden_size * self.directions * self.concat, self.hidden_size)\n",
        "        self.lin2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.lin3 = torch.nn.Linear(self.hidden_size, options['out_dim'])\n",
        "\n",
        "        for lin in [self.lin1, self.lin2, self.lin3]:\n",
        "            torch.nn.init.xavier_uniform_(lin.weight)\n",
        "            torch.nn.init.zeros_(lin.bias)\n",
        "\n",
        "        self.out = torch.nn.Sequential(\n",
        "            self.lin1,\n",
        "            self.relu,\n",
        "            self.dropout,\n",
        "            self.lin2,\n",
        "            self.relu,\n",
        "            self.dropout,\n",
        "            self.lin3\n",
        "        )\n",
        "\n",
        "        self.loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, premise, hypothesis, labels=None):\n",
        "        premise_embed = self.embedding(premise)\n",
        "        hypothesis_embed = self.embedding(hypothesis)\n",
        "\n",
        "        premise_proj = self.relu(self.projection(premise_embed))\n",
        "        hypothesis_proj = self.relu(self.projection(hypothesis_embed))\n",
        "\n",
        "        h0 = c0 = torch.tensor([]).new_zeros((self.num_layers * self.directions, premise.size(0), self.hidden_size)).to(self.device)\n",
        "\n",
        "        _, (premise_ht, _) = self.lstm(premise_proj, (h0, c0))\n",
        "        _, (hypothesis_ht, _) = self.lstm(hypothesis_proj, (h0, c0))\n",
        "\n",
        "        premise = premise_ht[-2:].transpose(0, 1).contiguous().view(premise.size(0), -1)\n",
        "        hypothesis = hypothesis_ht[-2:].transpose(0, 1).contiguous().view(premise.size(0), -1)\n",
        "\n",
        "        combined = torch.cat((premise, hypothesis, torch.abs(premise - hypothesis), premise * hypothesis), 1)\n",
        "        logits = self.out(combined)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        else:\n",
        "            return {\"logits\": logits}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EalNOqGRntHn"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "sToXgpQTTsTT"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, val_loader, config):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "        self.optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
        "        total_steps = len(train_loader) * config.num_epochs\n",
        "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    def evaluate(self, data_loader):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                outputs = self.model(**batch)\n",
        "                _, preds = torch.max(outputs[\"logits\"], dim=1)\n",
        "                self.metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
        "        return self.metric.compute(), outputs.get(\"loss\", None)\n",
        "\n",
        "    def train(self):\n",
        "        best_val_accuracy = 0.0\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            print(f'Epoch {epoch+1}/{self.config.num_epochs}')\n",
        "\n",
        "            self.metric = load_metric(\"accuracy\")\n",
        "\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                outputs = self.model(**batch)\n",
        "                loss = outputs[\"loss\"]\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "            # Validation\n",
        "            val_accuracy, val_loss = self.evaluate(self.val_loader)\n",
        "            if val_loss is not None:\n",
        "                print(f'Validation Loss: {val_loss}')\n",
        "            print(f'Validation Accuracy: {val_accuracy[\"accuracy\"]}')\n",
        "\n",
        "            # Save the best model separately\n",
        "            if val_accuracy[\"accuracy\"] > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy[\"accuracy\"]\n",
        "                print(f'New best validation accuracy: {best_val_accuracy}')\n",
        "                print(f'Saving model to {self.config.best_model_path}')\n",
        "                if isinstance(self.model, PreTrainedModel):\n",
        "                    self.model.save_pretrained(self.config.best_model_path)\n",
        "                else:\n",
        "                    torch.save(self.model.state_dict(), self.config.best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Ob_orTnzIb"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9xLUpWAn190"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRPWciWGn2j9"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "5m2lOOHPn3rq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/venkateshmurugadas/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:29<00:00,  1.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6939980983734131\n",
            "Validation Accuracy: 0.4931506849315068\n",
            "New best validation accuracy: 0.4931506849315068\n",
            "Saving model to best_model.pt\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:26<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6962475776672363\n",
            "Validation Accuracy: 0.4520547945205479\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:30<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6970223784446716\n",
            "Validation Accuracy: 0.4520547945205479\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:27<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6967896223068237\n",
            "Validation Accuracy: 0.4520547945205479\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:28<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6970521807670593\n",
            "Validation Accuracy: 0.4520547945205479\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:34<00:00,  1.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6968870759010315\n",
            "Validation Accuracy: 0.4520547945205479\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:39<00:00,  1.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6974933743476868\n",
            "Validation Accuracy: 0.4520547945205479\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:40<00:00,  1.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6977031230926514\n",
            "Validation Accuracy: 0.4520547945205479\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:39<00:00,  1.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6969847083091736\n",
            "Validation Accuracy: 0.4520547945205479\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 41/41 [00:40<00:00,  1.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6973340511322021\n",
            "Validation Accuracy: 0.4520547945205479\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'accuracy': 0.4691358024691358}, tensor(0.7201))"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the configuration\n",
        "class BiLSTMConfig:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    max_length = 128\n",
        "    batch_size = 16\n",
        "    learning_rate = 1e-5\n",
        "    num_epochs = 10\n",
        "    checkpoint_path = \"checkpoint.pt\"\n",
        "    best_model_path = \"best_model.pt\"\n",
        "    d_hidden = 128\n",
        "    dp_ratio = 0.1\n",
        "    out_dim = 2\n",
        "\n",
        "data_reader = DataReader(\"data/COLIEE2021statute_data-English/train\", 'data/coliee_train/coliee_2021.csv')\n",
        "data_reader.process()\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "glove = GloVe(name='6B', dim=300)\n",
        "vocab_size = len(glove.itos)\n",
        "\n",
        "csv_file = 'data/coliee_train/coliee_2021.csv'\n",
        "data_processor = BiLSTMDataProcessor(tokenizer, glove, BiLSTMConfig)\n",
        "train_loader, val_loader, test_loader = data_processor.get_data_loaders(csv_file)\n",
        "\n",
        "options = {'d_hidden': BiLSTMConfig.d_hidden, 'dp_ratio': BiLSTMConfig.dp_ratio, 'out_dim': BiLSTMConfig.out_dim, 'device': BiLSTMConfig.device, 'seed_value': 42}\n",
        "model = BiLSTM(vocab_size, options).to(BiLSTMConfig.device)\n",
        "trainer = Trainer(model, train_loader, val_loader, BiLSTMConfig)\n",
        "trainer.train()\n",
        "trainer.evaluate(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anqAdClPXPY2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP0etGMYXR7E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY_g33kxXjec"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
