{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "data = pd.read_csv(\"data/snli_train.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rwos with missing values and nan values\n",
    "\n",
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1: Distribution of the labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count of each label\n",
    "label_counts = data['label'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values, alpha=0.8)\n",
    "plt.title('Distribution of Labels in the Dataset')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Label', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 2: Average, minimum, and maximum length of the premises and hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word counts\n",
    "data['premise_word_count'] = data['premise'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data['hypothesis_word_count'] = data['hypothesis'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# Calculate statistics\n",
    "premise_word_count_stats = data['premise_word_count'].describe()\n",
    "hypothesis_word_count_stats = data['hypothesis_word_count'].describe()\n",
    "\n",
    "premise_word_count_stats, hypothesis_word_count_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 3: Correlation between the length of the premise and the hypothesis\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x='premise_word_count', y='hypothesis_word_count', data=data, alpha=0.5)\n",
    "plt.title('Correlation between the Length of the Premise and the Hypothesis')\n",
    "plt.xlabel('Premise Word Count')\n",
    "plt.ylabel('Hypothesis Word Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 4: Correlation between the length of the premise (or hypothesis) and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Premise\n",
    "sns.boxplot(x='label', y='premise_word_count', data=data, ax=ax[0])\n",
    "ax[0].set_title('Distribution of Premise Word Count by Label')\n",
    "ax[0].set_xlabel('Label')\n",
    "ax[0].set_ylabel('Premise Word Count')\n",
    "\n",
    "# Hypothesis\n",
    "sns.boxplot(x='label', y='hypothesis_word_count', data=data, ax=ax[1])\n",
    "ax[1].set_title('Distribution of Hypothesis Word Count by Label')\n",
    "ax[1].set_xlabel('Label')\n",
    "ax[1].set_ylabel('Hypothesis Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 5: Most common words in the premises and hypotheses for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and tokenize text\n",
    "def tokenize(text):\n",
    "    # Check if text is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercase and remove non-alphabetic characters\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Function to get most common words\n",
    "def get_most_common_words(texts, n=20):\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        words.extend(tokenize(text))\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "# Most common words for each label in the premise\n",
    "most_common_words_premise = data.groupby('label')['premise'].apply(get_most_common_words).reset_index()\n",
    "most_common_words_premise.columns = ['label', 'most_common_words_in_premise']\n",
    "\n",
    "# Most common words for each label in the hypothesis\n",
    "most_common_words_hypothesis = data.groupby('label')['hypothesis'].apply(get_most_common_words).reset_index()\n",
    "most_common_words_hypothesis.columns = ['label', 'most_common_words_in_hypothesis']\n",
    "\n",
    "most_common_words_premise, most_common_words_hypothesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot most common words for each label in the premise\n",
    "for label in most_common_words_premise['label'].unique():\n",
    "    word_counts_list = most_common_words_premise[most_common_words_premise['label'] == label]['most_common_words_in_premise'].to_list()[0]\n",
    "    words = [word for word, count in word_counts_list]\n",
    "    counts = [count for word, count in word_counts_list]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=words, y=counts)\n",
    "    plt.title(f'Most Common Words in Premise for Label: {label}')\n",
    "    plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "    plt.xlabel('Word', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot most common words for each label in the hypothesis\n",
    "for label in most_common_words_hypothesis['label'].unique():\n",
    "    word_counts_list = most_common_words_hypothesis[most_common_words_hypothesis['label'] == label]['most_common_words_in_hypothesis'].to_list()[0]\n",
    "    words = [word for word, count in word_counts_list]\n",
    "    counts = [count for word, count in word_counts_list]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=words, y=counts)\n",
    "    plt.title(f'Most Common Words in Premise for Label: {label}')\n",
    "    plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "    plt.xlabel('Word', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 6: Common bigrams or trigrams in the premises and hypotheses for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "\n",
    "def get_most_common_ngrams(texts, n=10, ngram=2):\n",
    "    # If ngram is 2, use bigrams\n",
    "    if ngram == 2:\n",
    "        ngram_function = bigrams\n",
    "    # If ngram is 3, use trigrams\n",
    "    elif ngram == 3:\n",
    "        ngram_function = trigrams\n",
    "    \n",
    "    ngram_list = []\n",
    "    for text in texts:\n",
    "        tokens = tokenize(text)  # Use the tokenize function defined earlier\n",
    "        ngram_list.extend(ngram_function(tokens))\n",
    "    return Counter(ngram_list).most_common(n)\n",
    "\n",
    "# Most common bigrams for each label in the premise\n",
    "most_common_bigrams_premise = data.groupby('label')['premise'].apply(lambda x: get_most_common_ngrams(x, ngram=2)).reset_index()\n",
    "most_common_bigrams_premise.columns = ['label', 'most_common_bigrams_in_premise']\n",
    "\n",
    "# Most common trigrams for each label in the hypothesis\n",
    "most_common_trigrams_hypothesis = data.groupby('label')['hypothesis'].apply(lambda x: get_most_common_ngrams(x, ngram=3)).reset_index()\n",
    "most_common_trigrams_hypothesis.columns = ['label', 'most_common_trigrams_in_hypothesis']\n",
    "\n",
    "print(most_common_bigrams_premise)\n",
    "print(most_common_trigrams_hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine bigrams with _ in between\n",
    "most_common_bigrams_premise['most_common_bigrams_in_premise'] = most_common_bigrams_premise['most_common_bigrams_in_premise'].apply(lambda x: [(' '.join(gram), count) for gram, count in x])\n",
    "\n",
    "# plot most common words for each label in the premise\n",
    "for label in most_common_bigrams_premise['label'].unique():\n",
    "    word_counts_list = most_common_bigrams_premise[most_common_bigrams_premise['label'] == label]['most_common_bigrams_in_premise'].to_list()[0]\n",
    "    words = [word for word, count in word_counts_list]\n",
    "    counts = [count for word, count in word_counts_list]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=words, y=counts)\n",
    "    plt.title(f'Most Common Words in Premise for Label: {label}')\n",
    "    plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "    plt.xlabel('Word', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine bigrams with _ in between\n",
    "most_common_trigrams_hypothesis['most_common_trigrams_in_hypothesis'] = most_common_trigrams_hypothesis['most_common_trigrams_in_hypothesis'].apply(lambda x: [(' '.join(gram), count) for gram, count in x])\n",
    "\n",
    "# plot most common words for each label in the premise\n",
    "for label in most_common_trigrams_hypothesis['label'].unique():\n",
    "    word_counts_list = most_common_trigrams_hypothesis[most_common_trigrams_hypothesis['label'] == label]['most_common_trigrams_in_hypothesis'].to_list()[0]\n",
    "    words = [word for word, count in word_counts_list]\n",
    "    counts = [count for word, count in word_counts_list]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=words, y=counts)\n",
    "    plt.title(f'Most Common Words in Premise for Label: {label}')\n",
    "    plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "    plt.xlabel('Word', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 7: Sentiment distribution of the premises and hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    # Get polarity from TextBlob\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Get sentiment for each premise and hypothesis\n",
    "data['premise_sentiment'] = data['premise'].apply(get_sentiment)\n",
    "data['hypothesis_sentiment'] = data['hypothesis'].apply(get_sentiment)\n",
    "\n",
    "# Plot sentiment distribution for premise\n",
    "sns.countplot(data=data, x='premise_sentiment')\n",
    "plt.title('Sentiment Distribution in Premise')\n",
    "plt.show()\n",
    "\n",
    "# Plot sentiment distribution for hypothesis\n",
    "sns.countplot(data=data, x='hypothesis_sentiment')\n",
    "plt.title('Sentiment Distribution in Hypothesis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 8: Correlation between the sentiment of the premise (or hypothesis) and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by sentiment and label, and count the number of premises for each combination\n",
    "premise_sentiment_label_counts = data.groupby(['premise_sentiment', 'label']).size().reset_index(name='counts')\n",
    "\n",
    "# Plot counts\n",
    "sns.catplot(data=premise_sentiment_label_counts, x='label', y='counts', hue='premise_sentiment', kind='bar')\n",
    "plt.title('Correlation between Premise Sentiment and Label')\n",
    "plt.show()\n",
    "\n",
    "# Repeat the same for hypothesis\n",
    "hypothesis_sentiment_label_counts = data.groupby(['hypothesis_sentiment', 'label']).size().reset_index(name='counts')\n",
    "\n",
    "# Plot counts\n",
    "sns.catplot(data=hypothesis_sentiment_label_counts, x='label', y='counts', hue='hypothesis_sentiment', kind='bar')\n",
    "plt.title('Correlation between Hypothesis Sentiment and Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 9: Number of unique words in the premises and hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique words in each premise and hypothesis\n",
    "data['premise_words'] = data['premise'].apply(lambda x: tokenize(x))\n",
    "data['hypothesis_words'] = data['hypothesis'].apply(lambda x: tokenize(x))\n",
    "\n",
    "premise_words = data['premise_words'].to_list()\n",
    "hypothesis_words = data['hypothesis_words'].to_list()\n",
    "\n",
    "# flatten list of lists\n",
    "premise_words = [word for sentence in premise_words for word in sentence]\n",
    "hypothesis_words = [word for sentence in hypothesis_words for word in sentence]\n",
    "words = premise_words + hypothesis_words\n",
    "\n",
    "# Get the number of unique words in each premise and hypothesis\n",
    "premise_word_counter = Counter(premise_words)\n",
    "hypothesis_word_counter = Counter(hypothesis_words)\n",
    "word_counter = Counter(words)\n",
    "\n",
    "print(\"Number of unique words in the dataset\", len(word_counter.keys()))\n",
    "print(\"Number of unique words in premise:\", len(premise_word_counter.keys()))\n",
    "print(\"Number of unique words in hypothesis:\", len(hypothesis_word_counter.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 10: Words that only appear in one type of label but not the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This operation can be resource-intensive for large datasets. Consider sampling or limiting the number of unique words if necessary.\n",
    "\n",
    "# Get all words for each label\n",
    "words_by_label = data.groupby('label')['premise'].apply(lambda texts: set(word for text in texts for word in tokenize(text)))\n",
    "\n",
    "# Find words that only appear in one label\n",
    "unique_words = {}\n",
    "for label in words_by_label.index:\n",
    "    other_labels = set(words_by_label.index) - {label}\n",
    "    other_words = set().union(*(words_by_label[other] for other in other_labels))\n",
    "    unique_words[label] = words_by_label[label] - other_words\n",
    "\n",
    "for label, words in unique_words.items():\n",
    "    print(f\"Unique words in {label}: {', '.join(list(words)[:10])}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 11: Lexical diversity in the premises and hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lexical diversity for each premise and hypothesis\n",
    "data['premise_lexical_diversity'] = data['premise_words'].apply(lambda x: len(set(x)) / len(x) if len(x) > 0 else 0)\n",
    "data['hypothesis_lexical_diversity'] = data['hypothesis_words'].apply(lambda x: len(set(x)) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "print('Average lexical diversity in premise:', data['premise_lexical_diversity'].mean())\n",
    "print('Average lexical diversity in hypothesis:', data['hypothesis_lexical_diversity'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 12: Distribution of part of speech tags in the premises and hypotheses for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to get POS tags\n",
    "def get_pos_tags(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.pos_ for token in doc]\n",
    "\n",
    "# Get POS tags for each premise and hypothesis\n",
    "data['premise_pos_tags'] = data['premise'].apply(get_pos_tags)\n",
    "data['hypothesis_pos_tags'] = data['hypothesis'].apply(get_pos_tags)\n",
    "\n",
    "# Get POS tag counts for each label\n",
    "pos_tag_counts_premise = data.explode('premise_pos_tags').groupby(['label', 'premise_pos_tags']).size().reset_index(name='counts')\n",
    "pos_tag_counts_hypothesis = data.explode('hypothesis_pos_tags').groupby(['label', 'hypothesis_pos_tags']).size().reset_index(name='counts')\n",
    "\n",
    "# Plot counts\n",
    "sns.catplot(data=pos_tag_counts_premise, x='label', y='counts', hue='premise_pos_tags', kind='bar')\n",
    "plt.title('POS Tag Counts in Premise by Label')\n",
    "plt.show()\n",
    "\n",
    "sns.catplot(data=pos_tag_counts_hypothesis, x='label', y='counts', hue='hypothesis_pos_tags', kind='bar')\n",
    "plt.title('POS Tag Counts in Hypothesis by Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 13: Words that are particularly indicative of a certain label (based on TF-IDF scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine premise and hypothesis\n",
    "data['text'] = data['premise'] + ' ' + data['hypothesis']\n",
    "\n",
    "# Calculate TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get top TF-IDF features for each label\n",
    "top_tfidf_features = {}\n",
    "for label in data['label'].unique():\n",
    "    # Get rows for this label\n",
    "    rows = data[data['label'] == label].index\n",
    "    # Sum TF-IDF scores for these rows\n",
    "    sum_tfidf = X[rows].sum(axis=0)\n",
    "    sum_tfidf = np.asarray(sum_tfidf).reshape(-1)\n",
    "    # Get top features\n",
    "    top_features_indices = sum_tfidf.argsort()[-10:][::-1]\n",
    "    top_features = [(feature_names[i], sum_tfidf[i]) for i in top_features_indices]\n",
    "    top_tfidf_features[label] = top_features\n",
    "\n",
    "top_tfidf_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 14: Readability score (like Flesch-Kincaid) of the premises and hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import textstat\n",
    "\n",
    "# Calculate readability score for each premise and hypothesis\n",
    "data['premise_readability'] = data['premise'].apply(textstat.flesch_reading_ease)\n",
    "data['hypothesis_readability'] = data['hypothesis'].apply(textstat.flesch_reading_ease)\n",
    "\n",
    "print('Average readability score in premise:', data['premise_readability'].mean())\n",
    "print('Average readability score in hypothesis:', data['hypothesis_readability'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 15: Premises and hypotheses containing named entities (persons, organizations, locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get named entities\n",
    "def get_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.label_ for ent in doc.ents]\n",
    "\n",
    "# Get named entities for each premise and hypothesis\n",
    "data['premise_named_entities'] = data['premise'].apply(get_named_entities)\n",
    "data['hypothesis_named_entities'] = data['hypothesis'].apply(get_named_entities)\n",
    "\n",
    "# Get named entity counts for each label\n",
    "named_entity_counts_premise = data.explode('premise_named_entities').groupby(['label', 'premise_named_entities']).size().reset_index(name='counts')\n",
    "named_entity_counts_hypothesis = data.explode('hypothesis_named_entities').groupby(['label', 'hypothesis_named_entities']).size().reset_index(name='counts')\n",
    "\n",
    "# Plot counts\n",
    "sns.catplot(data=named_entity_counts_premise, x='label', y='counts', hue='premise_named_entities', kind='bar')\n",
    "plt.title('Named Entity Counts in Premise by Label')\n",
    "plt.show()\n",
    "\n",
    "sns.catplot(data=named_entity_counts_hypothesis, x='label', y='counts', hue='hypothesis_named_entities', kind='bar')\n",
    "plt.title('Named Entity Counts in Hypothesis by Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 16: Correlation between the presence of named entities in the premise (or hypothesis) and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for whether each premise and hypothesis contains a named entity\n",
    "data['premise_contains_named_entity'] = data['premise_named_entities'].apply(lambda x: len(x) > 0)\n",
    "data['hypothesis_contains_named_entity'] = data['hypothesis_named_entities'].apply(lambda x: len(x) > 0)\n",
    "\n",
    "# Group by whether premise contains a named entity and label, and count the number of premises for each combination\n",
    "premise_ne_label_counts = data.groupby(['premise_contains_named_entity', 'label']).size().reset_index(name='counts')\n",
    "\n",
    "# Plot counts\n",
    "sns.catplot(data=premise_ne_label_counts, x='label', y='counts', hue='premise_contains_named_entity', kind='bar')\n",
    "plt.title('Counts of Premises Containing a Named Entity by Label')\n",
    "plt.show()\n",
    "\n",
    "# Repeat the same for hypothesis\n",
    "hypothesis_ne_label_counts = data.groupby(['hypothesis_contains_named_entity', 'label']).size().reset_index(name='counts')\n",
    "\n",
    "# Plot counts\n",
    "sns.catplot(data=hypothesis_ne_label_counts, x='label', y='counts', hue='hypothesis_contains_named_entity', kind='bar')\n",
    "plt.title('Counts of Hypotheses Containing a Named Entity by Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 17: Distribution of syntactic parse trees or dependency structures in the premises and hypotheses for each label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get root dependency\n",
    "def get_root_dependency(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.dep_ for token in doc if token.head == token]\n",
    "\n",
    "# Get root dependency for each premise and hypothesis\n",
    "data['premise_root_dependency'] = data['premise'].apply(get_root_dependency)\n",
    "data['hypothesis_root_dependency'] = data['hypothesis'].apply(get_root_dependency)\n",
    "\n",
    "# Get root dependency counts for each label\n",
    "root_dependency_counts_premise = data.explode('premise_root_dependency').groupby(['label', 'premise_root_dependency']).size().reset_index(name='counts')\n",
    "root_dependency_counts_hypothesis = data.explode('hypothesis_root_dependency').groupby(['label', 'hypothesis_root_dependency']).size().reset_index(name='counts')\n",
    "\n",
    "# Plot counts\n",
    "sns.catplot(data=root_dependency_counts_premise, x='label', y='counts', hue='premise_root_dependency', kind='bar')\n",
    "plt.title('Root Dependency Counts in Premise by Label')\n",
    "plt.show()\n",
    "\n",
    "sns.catplot(data=root_dependency_counts_hypothesis, x='label', y='counts', hue='hypothesis_root_dependency', kind='bar')\n",
    "plt.title('Root Dependency Counts in Hypothesis by Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 18: Distribution of semantic roles in the premises and hypotheses for each label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 19: Premises and hypotheses containing negations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to check if text contains a negation\n",
    "def contains_negation(text):\n",
    "    doc = nlp(text)\n",
    "    return any(token.dep_ == 'neg' for token in doc)\n",
    "\n",
    "# Check if each premise and hypothesis contains a negation\n",
    "data['premise_contains_negation'] = data['premise'].apply(contains_negation)\n",
    "data['hypothesis_contains_negation'] = data['hypothesis'].apply(contains_negation)\n",
    "\n",
    "print('Number of premises containing a negation:', data['premise_contains_negation'].sum())\n",
    "print('Number of hypotheses containing a negation:', data['hypothesis_contains_negation'].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 20: Correlation between the presence of negations in the premise (or hypothesis) and the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by whether premise contains a negation and label, and count the number of premises for each combination\n",
    "premise_neg_label_counts = data.groupby(['premise_contains_negation', 'label']).size().reset_index(name='counts')\n",
    "\n",
    "# Plot counts\n",
    "sns.catplot(data=premise_neg_label_counts, x='label', y='counts', hue='premise_contains_negation', kind='bar')\n",
    "plt.title('Counts of Premises Containing a Negation by Label')\n",
    "plt.show()\n",
    "\n",
    "# Repeat the same for hypothesis\n",
    "hypothesis_neg_label_counts = data.groupby(['hypothesis_contains_negation', 'label']).size().reset_index(name='counts')\n",
    "\n",
    "# Plot counts\n",
    "sns.catplot(data=hypothesis_neg_label_counts, x='label', y='counts', hue='hypothesis_contains_negation', kind='bar')\n",
    "plt.title('Counts of Hypotheses Containing a Negation by Label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
